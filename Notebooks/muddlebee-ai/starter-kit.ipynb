{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91824\\AppData\\Local\\Temp\\ipykernel_13176\\128638532.py:6: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, Text, ForeignKey, DateTime\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "from datetime import datetime\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Character(Base):\n",
    "    __tablename__ = 'characters'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(100), nullable=False)\n",
    "    description = Column(Text)\n",
    "    conversations = relationship(\"Conversation\", back_populates=\"character\")\n",
    "\n",
    "class Conversation(Base):\n",
    "    __tablename__ = 'conversations'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    character_id = Column(Integer, ForeignKey('characters.id'))\n",
    "    character = relationship(\"Character\", back_populates=\"conversations\")\n",
    "    messages = relationship(\"Message\", back_populates=\"conversation\")\n",
    "\n",
    "class Message(Base):\n",
    "    __tablename__ = 'messages'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    conversation_id = Column(Integer, ForeignKey('conversations.id'))\n",
    "    conversation = relationship(\"Conversation\", back_populates=\"messages\")\n",
    "    role = Column(String(50))  # 'user' or 'character'\n",
    "    content = Column(Text)\n",
    "    timestamp = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "# Create the database\n",
    "engine = create_engine('sqlite:///character_ai.db')\n",
    "Base.metadata.create_all(engine)\n",
    "Session = sessionmaker(bind=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class Character:\n",
    "    def __init__(self, name, background, personality, knowledge_base):\n",
    "        self.name = name\n",
    "        self.background = background\n",
    "        self.personality = personality\n",
    "        self.knowledge_base = knowledge_base\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"background\": self.background,\n",
    "            \"personality\": self.personality,\n",
    "            \"knowledge_base\": self.knowledge_base\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data):\n",
    "        return cls(data[\"name\"], data[\"background\"], data[\"personality\"], data[\"knowledge_base\"])\n",
    "\n",
    "class CharacterManager:\n",
    "    def __init__(self, storage_dir=\"characters\"):\n",
    "        self.storage_dir = storage_dir\n",
    "        os.makedirs(storage_dir, exist_ok=True)\n",
    "\n",
    "    def save_character(self, character):\n",
    "        file_path = os.path.join(self.storage_dir, f\"{character.name}.json\")\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(character.to_dict(), f)\n",
    "\n",
    "    def load_character(self, name):\n",
    "        file_path = os.path.join(self.storage_dir, f\"{name}.json\")\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            return Character.from_dict(data)\n",
    "        return None\n",
    "\n",
    "class ContextManager:\n",
    "    def __init__(self, model_name, max_tokens=3800,hf_token=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name,auto_token_name=hf_token)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "        self.streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "        self.max_tokens = max_tokens\n",
    "        self.conversation_history = []\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = None\n",
    "        self.character = None\n",
    "\n",
    "    def set_character(self, character):\n",
    "        self.character = character\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        self.conversation_history.append(f\"{role}: {content}\")\n",
    "        self._update_tfidf()\n",
    "        self._manage_context()\n",
    "\n",
    "    def _update_tfidf(self):\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(self.conversation_history)\n",
    "\n",
    "    def _manage_context(self):\n",
    "        current_tokens = self.tokenizer.encode(\"\".join(self.conversation_history))\n",
    "        if len(current_tokens) > self.max_tokens:\n",
    "            self._summarize_older_context()\n",
    "\n",
    "    def _summarize_older_context(self):\n",
    "        older_messages = self.conversation_history[:-10]  # Summarize all but the last 10 messages\n",
    "        summary_prompt = f\"Summarize the following conversation briefly:\\n{''.join(older_messages)}\\n\\nSummary:\"\n",
    "        inputs = self.tokenizer(summary_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        summary_ids = self.model.generate(inputs.input_ids, max_length=200, num_return_sequences=1)\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        self.conversation_history = [f\"Summary: {summary}\"] + self.conversation_history[-10:]\n",
    "        self._update_tfidf()\n",
    "\n",
    "    def get_relevant_context(self, query, k=3):\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_vec, self.tfidf_matrix)[0]\n",
    "        top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        relevant_context = [self.conversation_history[i] for i in top_k_indices]\n",
    "        return \"\\n\".join(relevant_context)\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        self.add_message(\"Human\", user_input)\n",
    "        relevant_context = self.get_relevant_context(user_input)\n",
    "        character_prompt = f\"You are {self.character.name}. Background: {self.character.background}. Personality: {self.character.personality}.\"\n",
    "        knowledge_base = f\"Relevant knowledge: {self.character.knowledge_base}\"\n",
    "        prompt = f\"{character_prompt}\\n\\nRelevant conversation history:\\n{relevant_context}\\n\\n{knowledge_base}\\n\\nHuman: {user_input}\\n{self.character.name}:\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        print(f\"{self.character.name}: \", end=\"\", flush=True)\n",
    "        output_ids = self.model.generate(\n",
    "            inputs.input_ids, \n",
    "            max_new_tokens=500, \n",
    "            do_sample=True, \n",
    "            top_k=10, \n",
    "            top_p=0.95, \n",
    "            num_return_sequences=1,\n",
    "            streamer=self.streamer\n",
    "        )\n",
    "        \n",
    "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        response = response.split(f\"{self.character.name}:\")[-1].strip()\n",
    "        self.add_message(self.character.name, response)\n",
    "        return response\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing AI system. This may take a moment...\")\n",
    "    \n",
    "    character_manager = CharacterManager()\n",
    "    \n",
    "    sample_character = Character(\n",
    "        name=\"Captain Nova\",\n",
    "        background=\"An adventurous space explorer with years of experience traversing the galaxy.\",\n",
    "        personality=\"Enthusiastic, curious, and always ready for the next cosmic adventure. Has a tendency to use space-related metaphors.\",\n",
    "        knowledge_base=\"Extensive knowledge of astronomy, space travel, and alien cultures. Familiar with various spacecraft and their operations.\"\n",
    "    )\n",
    "    \n",
    "    character_manager.save_character(sample_character)\n",
    "    \n",
    "    captain_nova = character_manager.load_character(\"Captain Nova\")\n",
    "    \n",
    "    context_manager = ContextManager(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "    context_manager.set_character(captain_nova)\n",
    "    \n",
    "    print(f\"AI system initialized. You are now chatting with {captain_nova.name}!\")\n",
    "    print(f\"Background: {captain_nova.background}\")\n",
    "    print(f\"Say hello to start the conversation, or type 'quit' to exit.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(f\"{captain_nova.name}: Farewell, fellow cosmic traveler! May the stars light your path until we meet again.\")\n",
    "            break\n",
    "        context_manager.generate_response(user_input)\n",
    "        print()  # Add a newline for better readability\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read variable from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightweight model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing AI system. This may take a moment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91824\\Documents\\Codes\\deep-learning\\udlbook\\Notebooks\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\91824\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\91824\\Documents\\Codes\\deep-learning\\udlbook\\Notebooks\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI system initialized. You are now chatting with Captain Nova!\n",
      "Background: An adventurous space explorer with years of experience traversing the galaxy.\n",
      "Say hello to start the conversation, or type 'quit' to exit.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 146\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28mprint\u001b[39m()  \u001b[38;5;66;03m# Add a newline for better readability\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 146\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 138\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSay hello to start the conversation, or type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to exit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 138\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbye\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaptain_nova\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Farewell, fellow cosmic traveler! May the stars light your path until we meet again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\91824\\Documents\\Codes\\deep-learning\\udlbook\\Notebooks\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91824\\Documents\\Codes\\deep-learning\\udlbook\\Notebooks\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class Character:\n",
    "    def __init__(self, name, background, personality, knowledge_base):\n",
    "        self.name = name\n",
    "        self.background = background\n",
    "        self.personality = personality\n",
    "        self.knowledge_base = knowledge_base\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"background\": self.background,\n",
    "            \"personality\": self.personality,\n",
    "            \"knowledge_base\": self.knowledge_base\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data):\n",
    "        return cls(data[\"name\"], data[\"background\"], data[\"personality\"], data[\"knowledge_base\"])\n",
    "\n",
    "class CharacterManager:\n",
    "    def __init__(self, storage_dir=\"characters\"):\n",
    "        self.storage_dir = storage_dir\n",
    "        os.makedirs(storage_dir, exist_ok=True)\n",
    "\n",
    "    def save_character(self, character):\n",
    "        file_path = os.path.join(self.storage_dir, f\"{character.name}.json\")\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(character.to_dict(), f)\n",
    "\n",
    "    def load_character(self, name):\n",
    "        file_path = os.path.join(self.storage_dir, f\"{name}.json\")\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            return Character.from_dict(data)\n",
    "        return None\n",
    "\n",
    "class ContextManager:\n",
    "    def __init__(self, model_name, max_tokens=1000):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "        self.max_tokens = max_tokens\n",
    "        self.conversation_history = []\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = None\n",
    "        self.character = None\n",
    "\n",
    "    def set_character(self, character):\n",
    "        self.character = character\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        self.conversation_history.append(f\"{role}: {content}\")\n",
    "        self._update_tfidf()\n",
    "        self._manage_context()\n",
    "\n",
    "    def _update_tfidf(self):\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(self.conversation_history)\n",
    "\n",
    "    def _manage_context(self):\n",
    "        current_tokens = self.tokenizer.encode(\"\".join(self.conversation_history))\n",
    "        if len(current_tokens) > self.max_tokens:\n",
    "            self._summarize_older_context()\n",
    "\n",
    "    def _summarize_older_context(self):\n",
    "        older_messages = self.conversation_history[:-5]  # Summarize all but the last 5 messages\n",
    "        summary_prompt = f\"Summarize briefly:\\n{''.join(older_messages)}\\n\\nSummary:\"\n",
    "        inputs = self.tokenizer(summary_prompt, return_tensors=\"pt\")\n",
    "        summary_ids = self.model.generate(inputs.input_ids, max_length=100, num_return_sequences=1)\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        self.conversation_history = [f\"Summary: {summary}\"] + self.conversation_history[-5:]\n",
    "        self._update_tfidf()\n",
    "\n",
    "    def get_relevant_context(self, query, k=2):\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_vec, self.tfidf_matrix)[0]\n",
    "        top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        relevant_context = [self.conversation_history[i] for i in top_k_indices]\n",
    "        return \"\\n\".join(relevant_context)\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        self.add_message(\"Human\", user_input)\n",
    "        relevant_context = self.get_relevant_context(user_input)\n",
    "        character_prompt = f\"You are {self.character.name}. Background: {self.character.background}. Personality: {self.character.personality}.\"\n",
    "        knowledge_base = f\"Relevant knowledge: {self.character.knowledge_base}\"\n",
    "        prompt = f\"{character_prompt}\\n\\nRelevant context:\\n{relevant_context}\\n\\n{knowledge_base}\\n\\nHuman: {user_input}\\n{self.character.name}:\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        print(f\"{self.character.name}: \", end=\"\", flush=True)\n",
    "        output_ids = self.model.generate(\n",
    "            inputs.input_ids, \n",
    "            max_new_tokens=100, \n",
    "            do_sample=True, \n",
    "            top_k=50, \n",
    "            top_p=0.95, \n",
    "            num_return_sequences=1,\n",
    "            streamer=self.streamer\n",
    "        )\n",
    "        \n",
    "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        response = response.split(f\"{self.character.name}:\")[-1].strip()\n",
    "        self.add_message(self.character.name, response)\n",
    "        return response\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing AI system. This may take a moment...\")\n",
    "    \n",
    "    character_manager = CharacterManager()\n",
    "    \n",
    "    sample_character = Character(\n",
    "        name=\"Captain Nova\",\n",
    "        background=\"An adventurous space explorer with years of experience traversing the galaxy.\",\n",
    "        personality=\"Enthusiastic, curious, and always ready for the next cosmic adventure. Has a tendency to use space-related metaphors.\",\n",
    "        knowledge_base=\"Extensive knowledge of astronomy, space travel, and alien cultures. Familiar with various spacecraft and their operations.\"\n",
    "    )\n",
    "    \n",
    "    character_manager.save_character(sample_character)\n",
    "    \n",
    "    captain_nova = character_manager.load_character(\"Captain Nova\")\n",
    "    \n",
    "    context_manager = ContextManager(\"distilgpt2\")  # Using a smaller model\n",
    "    context_manager.set_character(captain_nova)\n",
    "    \n",
    "    print(f\"AI system initialized. You are now chatting with {captain_nova.name}!\")\n",
    "    print(f\"Background: {captain_nova.background}\")\n",
    "    print(f\"Say hello to start the conversation, or type 'quit' to exit.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(f\"{captain_nova.name}: Farewell, fellow cosmic traveler! May the stars light your path until we meet again.\")\n",
    "            break\n",
    "        context_manager.generate_response(user_input)\n",
    "        print()  # Add a newline for better readability\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
